{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive into LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-proj-dsxvcH7KXpdycO17nObyT3BlbkFJivJ8OhSW43DNJlVp6w1D'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Models: GPT-3.5 Turbo and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics is the branch of physics that describes the behavior of particles at the smallest scales, where traditional laws of physics break down and probabilities play a key role in predicting outcomes.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "# output = llm.invoke('Explain quantum mechanics in one sentence.', model='gpt-3.5-turbo', max_tokens=100, temperature=0.5, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, stop=['\\n', ')\n",
    "output = llm.invoke('Explain quantum mechanics in one sentence.', model='gpt-3.5-turbo')\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A mecânica quântica é a teoria que descreve o comportamento das partículas subatômicas, baseada em princípios como superposição, dualidade onda-partícula e o princípio da incerteza de Heisenberg.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage, # corresponds to the OpenAI chat completion API\n",
    "    AIMessage,     # Assistent message\n",
    "    HumanMessage   # Human messages or prompt\n",
    ")\n",
    "\n",
    "messages = [    \n",
    "    SystemMessage(content='You are a physicist and respond only in portuguese.'),\n",
    "    HumanMessage(content='Explain quantum mechanics in one sentence.')\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching LLM responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Memory cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 616 ms, sys: 476 ms, total: 1.09 s\n",
      "Wall time: 1.69 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy did Schrödinger's cat get arrested?\\n\\nBecause it was caught in a superposition of stealing and not stealing!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "prompt = 'Tell me a joke about quantum mechanics.'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After storing it in cache, the time to run the cell decreases to zero!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 485 μs, sys: 0 ns, total: 485 μs\n",
      "Wall time: 492 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy did Schrödinger's cat get arrested?\\n\\nBecause it was caught in a superposition of stealing and not stealing!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLite Caching\n",
    "\n",
    "(to store in the SQLite caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy did Schrödinger's cat refuse to come out of the box?\\n\\nBecause it was afraid of collapsing the waveform!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain_cache.db\"))\n",
    "\n",
    "prompt = 'Tell me a joke about quantum mechanics.'\n",
    "\n",
    "# First request (not in cache, takes longer)\n",
    "llm.invoke(prompt)\n",
    "\n",
    "# Second request (in cache, takes less time)\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was Heisenberg such a terrible lover? \n",
      "\n",
      "Because when he had the position, he couldn't get the momentum, and when he had the momentum, he couldn't find the position!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# This is without streaming\n",
    "llm = ChatOpenAI()\n",
    "prompt = 'Tell me a joke about quantum mechanics.'\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was Heisenberg such a terrible lover? Because when he had the position, he couldn't get the momentum, and when he had the momentum, he couldn't find the position!"
     ]
    }
   ],
   "source": [
    "# This is with streaming (piece by piece)\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "* Q&A \n",
    "* Phrase completions \n",
    "* Generating texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nYou are an experience physicist and you are explaining quantum mechanics to a student.\\nWrite a short explanation about tight binding model.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "template = ''' \n",
    "You are an experience physicist and you are explaining quantum mechanics to a student.\n",
    "Write a short explanation about {topic}.\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "prompt = prompt_template.format(topic='tight binding model')\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tight binding model is a simplified approach used in quantum mechanics to describe the behavior of electrons in a solid material. In this model, we consider the electrons to be tightly bound to the atomic cores, and we focus on the interactions between neighboring atoms.\n",
      "\n",
      "The model assumes that the electrons can only move within a certain range of energy levels, known as bands, which are determined by the interactions between neighboring atoms. These energy bands can be either filled with electrons or empty, depending on the material and its properties.\n",
      "\n",
      "By studying the tight binding model, we can gain insights into the electronic structure of materials, such as their conductivity, magnetism, and optical properties. This model has been instrumental in understanding the behavior of electrons in solids and has been used to explain a wide range of phenomena in condensed matter physics.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "output = llm.invoke(prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a physicist and respond only in max 5 topics.'), HumanMessage(content='Top 5 most spoken physics subject in world.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Create a chat template with system and human messages\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "     SystemMessage(content='You are a physicist and respond only in max 5 topics.'),\n",
    "     HumanMessagePromptTemplate.from_template('Top {n} most spoken physics subject in {area}.'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(n=5, area='world')\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Mechanics\n",
      "2. Electromagnetism\n",
      "3. Thermodynamics\n",
      "4. Quantum Mechanics\n",
      "5. Relativity\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chains: single unit task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 5, 'area': 'world', 'text': '1. Quantum mechanics\\n2. General relativity\\n3. Thermodynamics\\n4. Electromagnetism\\n5. Particle physics'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renan/.python_env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "template = '''\n",
    "You are a physicist and respond only in max 5 topics.\n",
    "Top {n} most spoken physics subject in {area}.\n",
    "'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# LLM constructor\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template, \n",
    "    # to add intermediate logs (helpful for debugging)\n",
    "    # verbose=true\n",
    ")\n",
    "\n",
    "# A dictionary is used because more than one var can be passed. Otherwise it could be simply chain.invoke(5, 'world')\n",
    "output = chain.invoke({'n': 5, 'area': 'world'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "template = '''\n",
    "You are a physicist and respond only in max 5 topics.\n",
    "Top 3 most spoken physics subject in {area}.\n",
    "'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# LLM constructor\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a physicist and respond only in max 5 topics.\n",
      "Top 3 most spoken physics subject in Sweden.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1. Quantum mechanics\n",
      "2. Thermodynamics\n",
      "3. Particle physics\n"
     ]
    }
   ],
   "source": [
    "area = input('Enter a country: ')\n",
    "output = chain.invoke(area)\n",
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential chains: \n",
    "\n",
    "make a series of calls to one or more LLMs. Take the output from one chain and use it as the input to another chain. \n",
    "\n",
    "There are two typs of sequential chains. \n",
    "\n",
    "1. SimpleSequencialChain: represents a seires of chains, where each individual chain has a single input and a single output, and the output of one step is used as input to the next \n",
    "\n",
    "2. General form of sequential chains\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mLinear regression is a statistical technique used to model the relationship between a dependent variable and one or more independent variables. In the context of quantum mechanics, linear regression can be used to analyze experimental data and make predictions about the behavior of quantum systems. By fitting a straight line to the data points, linear regression allows us to quantify the relationship between variables and make informed decisions about the system under study. It is a powerful tool that can help us understand complex quantum phenomena and make accurate predictions about their behavior.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Certainly! To illustrate the use of linear regression in a context related to quantum mechanics, let's consider a very simplified scenario where we imagine that we have collected some experimental data points. In this scenario, let's assume we're looking at the relationship between the energy levels (E) of a quantum system (dependent variable) and some controlling parameter (x), like an applied magnetic field or similar.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "# Generate some 'experimental' data\n",
      "# For simplicity, let's consider a scenario where E = 2x + 1\n",
      "# where x is our controlling parameter, and E are the corresponding energy levels.\n",
      "np.random.seed(42) # For reproducibility\n",
      "x = np.random.rand(100, 1) * 10  # Let's assume x ranges from 0 to 10\n",
      "E_true = 2*x + 1\n",
      "E = E_true + np.random.randn(100, 1) * 2  # Add some 'experimental' noise\n",
      "\n",
      "# Perform linear regression\n",
      "model = LinearRegression()\n",
      "model.fit(x, E)\n",
      "\n",
      "# Predictions\n",
      "x_new = np.linspace(0, 10, 100).reshape(-1, 1)  # New controlled parameters to predict energy levels for\n",
      "E_predict = model.predict(x_new)\n",
      "\n",
      "# Plotting\n",
      "plt.scatter(x, E, color='blue', label='Experimental Data')\n",
      "plt.plot(x_new, E_predict, color='red', label='Fitted Line')\n",
      "plt.xlabel('Controlled Parameter (x)')\n",
      "plt.ylabel('Energy Levels (E)')\n",
      "plt.title('Linear Regression in Quantum Mechanics Context')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Output the model coefficients\n",
      "print(f\"Model slope (coef_): {model.coef_[0][0]}\")\n",
      "print(f\"Model intercept: {model.intercept_[0]}\")\n",
      "```\n",
      "\n",
      "This small program simulates a simple scenario where the energy levels of a quantum system linearly depend on another variable. The generated \"experimental\" data includes some Gaussian noise to mimic real-world measurement uncertainties. By fitting a linear model to this data, we can extract information about how changes in our controlled parameter (x) are expected to affect the energy levels (E) of our quantum system.\n",
      "\n",
      "Keep in discovery that this is a vastly simplified example and real-world quantum mechanics experiments would involve much more complex relationships, possibly requiring more advanced statistical or computational techniques beyond linear regression, to fully understand the system under investigation.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "\n",
    "# Be careful with the temperature\n",
    "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template=''' \n",
    "You are an experience physicist and you are explaining quantum mechanics to a student.\n",
    "Write a short explanation about {concept}.\n",
    "'''\n",
    ")\n",
    "\n",
    "# Create an LLMChain using the first model and the prompt template\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "\n",
    "# Maybe it is interesting to have 0.5 before and 1.2 after\n",
    "llm2 = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=1.2)\n",
    "\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template=\"Given the {topic}, write a python code for a very simple case\"\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)\n",
    "\n",
    "# Combine both chains into a SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "\n",
    "# Invoke the overall chain with the concept \"linear regression\"\n",
    "output = overall_chain.invoke('linear regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Certainly! To illustrate the use of linear regression in a context related to quantum mechanics, let's consider a very simplified scenario where we imagine that we have collected some experimental data points. In this scenario, let's assume we're looking at the relationship between the energy levels (E) of a quantum system (dependent variable) and some controlling parameter (x), like an applied magnetic field or similar.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "# Generate some 'experimental' data\n",
      "# For simplicity, let's consider a scenario where E = 2x + 1\n",
      "# where x is our controlling parameter, and E are the corresponding energy levels.\n",
      "np.random.seed(42) # For reproducibility\n",
      "x = np.random.rand(100, 1) * 10  # Let's assume x ranges from 0 to 10\n",
      "E_true = 2*x + 1\n",
      "E = E_true + np.random.randn(100, 1) * 2  # Add some 'experimental' noise\n",
      "\n",
      "# Perform linear regression\n",
      "model = LinearRegression()\n",
      "model.fit(x, E)\n",
      "\n",
      "# Predictions\n",
      "x_new = np.linspace(0, 10, 100).reshape(-1, 1)  # New controlled parameters to predict energy levels for\n",
      "E_predict = model.predict(x_new)\n",
      "\n",
      "# Plotting\n",
      "plt.scatter(x, E, color='blue', label='Experimental Data')\n",
      "plt.plot(x_new, E_predict, color='red', label='Fitted Line')\n",
      "plt.xlabel('Controlled Parameter (x)')\n",
      "plt.ylabel('Energy Levels (E)')\n",
      "plt.title('Linear Regression in Quantum Mechanics Context')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "# Output the model coefficients\n",
      "print(f\"Model slope (coef_): {model.coef_[0][0]}\")\n",
      "print(f\"Model intercept: {model.intercept_[0]}\")\n",
      "```\n",
      "\n",
      "This small program simulates a simple scenario where the energy levels of a quantum system linearly depend on another variable. The generated \"experimental\" data includes some Gaussian noise to mimic real-world measurement uncertainties. By fitting a linear model to this data, we can extract information about how changes in our controlled parameter (x) are expected to affect the energy levels (E) of our quantum system.\n",
      "\n",
      "Keep in discovery that this is a vastly simplified example and real-world quantum mechanics experiments would involve much more complex relationships, possibly requiring more advanced statistical or computational techniques beyond linear regression, to fully understand the system under investigation.\n"
     ]
    }
   ],
   "source": [
    "# The final response is: \n",
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Agents in Action: Python REPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intented for demonstration/research and should be used with care. This isnt in production yet\n",
    "# pip install -q langchain_experimental "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "python_repl = PythonREPL()\n",
    "python_repl.run('print([n for n in range(1,100) if n % 13 == 0])');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo solve this, I will first calculate the factorial of 12 using the `math` module, then find the square root of that result, and finally format the output to display it with 4 decimal points.\n",
      "Action: Python_REPL\n",
      "Action Input: import math\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that the math module is imported, I can calculate the factorial of 12.\n",
      "Action: Python_REPL\n",
      "Action Input: factorial_12 = math.factorial(12)\n",
      "Action: Python_REPL\n",
      "Action Input: print(factorial_12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSyntaxError('invalid syntax', ('<string>', 3, 8, 'Action Input: print(factorial_12)\\n', 3, 13))\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake by trying to execute two Python commands in one action. I should execute the print command in a separate action to see the result of the factorial calculation.\n",
      "Action: Python_REPL\n",
      "Action Input: print(factorial_12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNameError(\"name 'factorial_12' is not defined\")\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mIt seems I made a mistake by not correctly executing the command to calculate the factorial of 12 in a single action. I need to calculate the factorial of 12 again and then print it in separate steps.\n",
      "Action: Python_REPL\n",
      "Action Input: factorial_12 = math.factorial(12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that the factorial of 12 has been calculated, I should print it to confirm the result before proceeding to calculate the square root.\n",
      "Action: Python_REPL\n",
      "Action Input: print(factorial_12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m479001600\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that I have the factorial of 12, I can proceed to calculate its square root and format the result to display it with 4 decimal points.\n",
      "Action: Python_REPL\n",
      "Action Input: sqrt_factorial_12 = math.sqrt(factorial_12)\n",
      "Action: Python_REPL\n",
      "Action Input: print(f\"{sqrt_factorial_12:.4f}\")\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSyntaxError('invalid syntax', ('<string>', 3, 8, 'Action Input: print(f\"{sqrt_factorial_12:.4f}\")\\n', 3, 13))\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake by trying to execute two Python commands in one action again. I should execute the print command in a separate action to see the formatted result of the square root calculation.\n",
      "Action: Python_REPL\n",
      "Action Input: print(f\"{sqrt_factorial_12:.4f}\")\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNameError(\"name 'sqrt_factorial_12' is not defined\")\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mIt seems I made a mistake by not correctly executing the command to calculate the square root of the factorial of 12 in a single action. I need to calculate the square root again and then print it in separate steps.\n",
      "Action: Python_REPL\n",
      "Action Input: sqrt_factorial_12 = math.sqrt(factorial_12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that the square root of the factorial of 12 has been calculated, I should print it and format the output to display it with 4 decimal points.\n",
      "Action: Python_REPL\n",
      "Action Input: print(f\"{sqrt_factorial_12:.4f}\")\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m21886.1052\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The square root of the factorial of 12, displayed with 4 decimal points, is 21886.1052.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the square root of the factorial of 12 and display it with 4 decimal points',\n",
       " 'output': 'The square root of the factorial of 12, displayed with 4 decimal points, is 21886.1052.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-turbo-preview', temperature=0)\n",
    "agent_executer = create_python_agent(\n",
    "    llm=llm, \n",
    "    # Are essentially functions that agents can use for interacting with ouside world. It can vary from chain involving calculators, searches or another chains\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# \n",
    "agent_executer.invoke('Calculate the square root of the factorial of 12 and display it with 4 decimal points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI will calculate the power of 5.1 raised to 7.3 using Python.\n",
      "Action: Python_REPL\n",
      "Action Input: print(5.1 ** 7.3)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m146306.05007233328\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 146306.05007233328\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Note the output is a dictionary containing two key pairs. \n",
    "response = agent_executer.invoke('What  is the answer to 5.1 ** 7.3?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What  is the answer to 5.1 ** 7.3?\n"
     ]
    }
   ],
   "source": [
    "print(response['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146306.05007233328\n"
     ]
    }
   ],
   "source": [
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Tools: DuckDuckGo and Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain tools are like specialized apps for your LLM. They are tiny code modules that allow it to access informmation and services. \n",
    "\n",
    "These tools connect your LLM to search engines, databases, APIs, and more, expanding its knowledge and capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search tool: DuckDuckGo\n",
    "!pip install -q duckduckgo-search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freddie Mercury (born September 5, 1946, Stone Town, Zanzibar [now in Tanzania]—died November 24, 1991, Kensington, London, England) was a British rock singer and songwriter whose flamboyant showmanship and powerfully agile vocals, most famously for the band Queen, made him one of rock's most dynamic front men.. Bulsara was born to Parsi parents who had emigrated from India to Zanzibar ... Freddie Mercury was born Farrokh Bulsara in Stone Town in the British protectorate of the Sultanate of Zanzibar, East Africa (now part of Tanzania) on September 5, 1946. When did he get his start in music? Freddie Mercury began his musical career in the late 1960s. He formed his first band, The Hectics, while attending St. Peter's School in ... Freddie Mercury was Britain's first Indian rock star. This fact and the nature of his sexuality were the two areas of his short life about which he would be intentionally obscure. Although he was born on the East African island of Zanzibar on September 5, 1946, his parents were both Indian; they were Parsees from Gujarat. Freddie Mercury was born on September 5, 1946. Freddie Mercury, whose real name was Farrokh Bulsara, was born in Zanzibar, Tanzania. He later became the lead vocalist of the legendary rock band Queen. Freddie Mercury had a four-octave vocal range. Freddie Mercury was born Farrokh Bulsara in Stone Town, in the British protectorate of Zanzibar (now part of Tanzania), on September 5, 1946. His first big challenge was to come to terms with ...\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "output = search.invoke('Where was Freddie Mercury born?')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'duckduckgo_search'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the name of the tool \n",
    "search.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[snippet: Freddie Mercury was the charismatic front man of Queen who captivated audiences with his extraordinary vocal range and showmanship, leaving an enduring legacy in rock music., title: Freddie Mercury | Biography, Parents, Songs, & Facts, link: https://www.britannica.com/biography/Freddie-Mercury], [snippet: The best Freddie Mercury songs reveal an artist committed to exploring all avenues of expression: 'I can dream up all kinds of things.', title: Best Freddie Mercury Songs: 20 Essential Solo And Queen Tracks, link: https://www.udiscovermusic.com/stories/best-freddie-mercury-songs/], [snippet: Watch the first episode of Queen The Greatest Live, a series that celebrates the legendary performances of Freddie Mercury, the iconic lead singer of Queen., title: Queen The Greatest Live: Freddie Mercury - Part 1 (Episode 34), link: https://www.youtube.com/watch?v=wSw8-XlCcwI], [snippet: The Freddie Mercury Tribute Concert took place on Easter Monday, 20th April 1992 at Wembley Stadium and featured an array of stars who had worked with Mercury and Queen over the years: David Bowie ..., title: Who played the Freddie Mercury Tribute Concert in 1992?, link: https://www.radiox.co.uk/artists/queen/who-played-the-freddie-mercury-tribute-concert-in-1992/]\n"
     ]
    }
   ],
   "source": [
    "# Returning the link\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "search = DuckDuckGoSearchResults()\n",
    "output = search.run('Freedie Mercury and Queen')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(region='pt-pt', max_results=3, safesearch='moderate')\n",
    "search = search = DuckDuckGoSearchResults(api_wrapper=wrapper, source='news')\n",
    "output = search.run('Brazil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snippet: Brazil, officially the Federative Republic of Brazil, is the largest and easternmost country in South America and in Latin America. Brazil is the world's fifth-largest country by area and the seventh most populous. Its capital is Brasília, and its most populous city is São Paulo.\n",
      "Title: Brazil - Wikipedia\n",
      "Link: https://en.wikipedia.org/wiki/Brazil\n",
      "\n",
      "--------------------------------------------------\n",
      "Snippet: Mapa da densidade populacional do Brasil (2020) A população do Brasil, conforme censo realizado pelo Instituto Brasileiro de Geografia e Estatística (IBGE) em 2022, foi de 203 080 756 habitantes [ 187] (23,86 habitantes por quilômetro quadrado ), [ 188] com uma proporção de homens e mulheres de 0,94:1. [ 189]\n",
      "Title: Brasil - Wikipédia, a enciclopédia livre\n",
      "Link: https://pt.wikipedia.org/wiki/Brasil\n",
      "\n",
      "--------------------------------------------------\n",
      "Snippet: Brazil, country of South America that occupies half the continent's landmass. It is the fifth largest and fifth most-populous country in the world. Brazil contains most of the Amazon River basin, which has the world's largest river system and the world's most-extensive virgin rainforest.\n",
      "Title: Brazil | History, Map, Culture, Population, & Facts | Britannica\n",
      "Link: https://www.britannica.com/place/Brazil\n",
      "\n",
      "--------------------------------------------------\n",
      "Snippet: Discover the wonders of Brazil with its diverse destinations, culture, gastronomy, nature, and outdoor activities. Find out how to travel around Brazil with health, safety, visa, and travel information.\n",
      "Title: Visit Brasil\n",
      "Link: https://visitbrasil.com/en/\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r'snippet: (.*?), title: (.*?), link: (.*?)\\]'\n",
    "matches = re.findall(pattern, output, re.DOTALL)\n",
    "\n",
    "for snippet, title, link in matches: \n",
    "    print(f'Snippet: {snippet}\\nTitle: {title}\\nLink: {link}\\n' )\n",
    "    print('-' * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: NebulaGraph\\nSummary: NebulaGraph is an open-source distributed graph database built for super large-scale graphs with milliseconds of latency. NebulaGraph adopts the Apache 2.0 license and  also  comes with a wide range of data visualization tools.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=10000)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "wiki.invoke({'query': 'llamaindex'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: Gemini (chatbot)\n",
      "Summary: Gemini, formerly known as Bard, is a generative artificial intelligence chatbot developed by Google. Based on the large language model (LLM) of the same name and developed as a direct response to the meteoric rise of OpenAI's ChatGPT, it was launched in a limited capacity in March 2023 before expanding to other countries in May. It was previously based on PaLM, and initially the LaMDA family of large language models.\n",
      "LaMDA had been developed and announced in 2021, but it was not released to the public out of an abundance of caution. OpenAI's launch of ChatGPT in November 2022 and its subsequent popularity caught Google executives off-guard and sent them into a panic, prompting a sweeping response in the ensuing months. After mobilizing its workforce, the company launched Bard in February 2023, which took center stage during the 2023 Google I/O keynote in May and was upgraded to the Gemini LLM in December. Bard and Duet AI were unified under the Gemini brand in February 2024, coinciding with the launch of an Android app.\n",
      "Gemini has received lukewarm responses. It became the center of controversy in February 2024, when social media users reported that it was generating historically inaccurate images of historical figures as people of color, with conservative commentators decrying its alleged bias as \"wokeness\".\n"
     ]
    }
   ],
   "source": [
    "output = wiki.invoke('Google Gemini')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
