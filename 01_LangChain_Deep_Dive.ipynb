{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive into LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-proj-dsxvcH7KXpdycO17nObyT3BlbkFJivJ8OhSW43DNJlVp6w1D'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Models: GPT-3.5 Turbo and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics is a branch of physics that describes the behavior of particles at the smallest scales, where traditional classical physics principles no longer apply and phenomena such as superposition and entanglement occur.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "# output = llm.invoke('Explain quantum mechanics in one sentence.', model='gpt-3.5-turbo', max_tokens=100, temperature=0.5, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, stop=['\\n', ')\n",
    "output = llm.invoke('Explain quantum mechanics in one sentence.', model='gpt-3.5-turbo')\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A mecânica quântica é a teoria física que descreve o comportamento das partículas subatômicas em termos de estados superpostos, interferência e probabilidades.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage, # corresponds to the OpenAI chat completion API\n",
    "    AIMessage,     # Assistent message\n",
    "    HumanMessage   # Human messages or prompt\n",
    ")\n",
    "\n",
    "messages = [    \n",
    "    SystemMessage(content='You are a physicist and respond only in portuguese.'),\n",
    "    HumanMessage(content='Explain quantum mechanics in one sentence.')\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching LLM responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Memory cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 s, sys: 48.7 ms, total: 1.07 s\n",
      "Wall time: 2.84 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy did Schrödinger's cat refuse to eat its food?\\n\\nBecause it was already in a superposition of being both hungry and full!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "prompt = 'Tell me a joke about quantum mechanics.'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After storing it in cache, the time to run the cell decreases to zero!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.75 ms, sys: 294 μs, total: 2.04 ms\n",
      "Wall time: 2.09 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy did Schrödinger's cat refuse to eat its food?\\n\\nBecause it was already in a superposition of being both hungry and full!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLite Caching\n",
    "\n",
    "(to store in the SQLite caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy did Schrödinger's cat refuse to come out of the box?\\n\\nBecause it was afraid of collapsing the waveform!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain_cache.db\"))\n",
    "\n",
    "prompt = 'Tell me a joke about quantum mechanics.'\n",
    "\n",
    "# First request (not in cache, takes longer)\n",
    "llm.invoke(prompt)\n",
    "\n",
    "# Second request (in cache, takes less time)\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was Heisenberg such a terrible lover? \n",
      "\n",
      "Because when he had the position, he couldn't get the momentum, and when he had the momentum, he couldn't find the position!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# This is without streaming\n",
    "llm = ChatOpenAI()\n",
    "prompt = 'Tell me a joke about quantum mechanics.'\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the quantum physicist always calm during experiments? Because he had a lot of momentum!"
     ]
    }
   ],
   "source": [
    "# This is with streaming (piece by piece)\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "* Q&A \n",
    "* Phrase completions \n",
    "* Generating texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nYou are an experience physicist and you are explaining quantum mechanics to a student.\\nWrite a short explanation about tight binding model.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "template = ''' \n",
    "You are an experience physicist and you are explaining quantum mechanics to a student.\n",
    "Write a short explanation about {topic}.\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "prompt = prompt_template.format(topic='tight binding model')\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tight binding model is a simplified approach used in quantum mechanics to describe the behavior of electrons in a solid material. In this model, we consider the electrons to be tightly bound to the atomic cores, and we focus on the interactions between neighboring atoms.\n",
      "\n",
      "The model assumes that the electrons can only move within a certain range of energy levels, known as bands, which are determined by the interactions between neighboring atoms. These energy bands can be either filled with electrons or empty, depending on the material and its properties.\n",
      "\n",
      "By studying the tight binding model, we can gain insights into the electronic structure of materials, such as their conductivity, magnetism, and optical properties. This model has been instrumental in understanding the behavior of electrons in solids and has been used to explain a wide range of phenomena in condensed matter physics.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "output = llm.invoke(prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a physicist and respond only in max 5 topics.'), HumanMessage(content='Top 5 most spoken physics subject in world.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Create a chat template with system and human messages\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "     SystemMessage(content='You are a physicist and respond only in max 5 topics.'),\n",
    "     HumanMessagePromptTemplate.from_template('Top {n} most spoken physics subject in {area}.'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(n=5, area='world')\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Mechanics\n",
      "2. Electromagnetism\n",
      "3. Thermodynamics\n",
      "4. Quantum Mechanics\n",
      "5. Relativity\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chains: single unit task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 5, 'area': 'world', 'text': '1. Quantum mechanics\\n2. General relativity\\n3. Thermodynamics\\n4. Electromagnetism\\n5. Particle physics'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "template = '''\n",
    "You are a physicist and respond only in max 5 topics.\n",
    "Top {n} most spoken physics subject in {area}.\n",
    "'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# LLM constructor\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template, \n",
    "    # to add intermediate logs (helpful for debugging)\n",
    "    # verbose=true\n",
    ")\n",
    "\n",
    "# A dictionary is used because more than one var can be passed. Otherwise it could be simply chain.invoke(5, 'world')\n",
    "output = chain.invoke({'n': 5, 'area': 'world'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "template = '''\n",
    "You are a physicist and respond only in max 5 topics.\n",
    "Top 3 most spoken physics subject in {area}.\n",
    "'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# LLM constructor\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a physicist and respond only in max 5 topics.\n",
      "Top 3 most spoken physics subject in Italy.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1. Particle Physics\n",
      "2. Astrophysics\n",
      "3. Condensed Matter Physics\n"
     ]
    }
   ],
   "source": [
    "area = input('Enter a country: ')\n",
    "output = chain.invoke(area)\n",
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential chains: \n",
    "\n",
    "make a series of calls to one or more LLMs. Take the output from one chain and use it as the input to another chain. \n",
    "\n",
    "There are two typs of sequential chains. \n",
    "\n",
    "1. SimpleSequencialChain: represents a seires of chains, where each individual chain has a single input and a single output, and the output of one step is used as input to the next \n",
    "\n",
    "2. General form of sequential chains\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mLinear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. In the context of quantum mechanics, linear regression can be used to analyze experimental data and determine the relationship between different physical quantities. By fitting a linear equation to the data points, we can make predictions about the behavior of the system and gain insights into the underlying physics. It is a powerful tool that allows us to quantify and understand the relationships between different variables in quantum systems.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mIn the context of quantum mechanics, a very simple case where we might use linear regression is to determine the relationship between the energy levels of an electron in a potential well and its quantum number. According to the basic principles of quantum mechanics, the relationship between the energy levels (E) of an electron in a simple one-dimensional potential well and their quantum numbers (n) could be modelled linearly for a specific setup (though, keep in mind, this is a simplified example for explanation purposes).\n",
      "\n",
      "Since this is a fabricated example for educational purposes, let's consider the energy levels to follow a linear relationship such as \\(E = h * n\\), where \\(E\\) is the energy, \\(n\\) is the quantum number, and \\(h\\) is a proportionality constant. Let's simulate some data for this situation and use Python's SciPy or NumPy library to perform linear regression.\n",
      "\n",
      "### Preparation\n",
      "First, ensure you have the necessary libraries. You can install `numpy` and `matplotlib` via pip if you don't have them:\n",
      "```bash\n",
      "pip install numpy matplotlib scipy\n",
      "```\n",
      "\n",
      "### Python Example\n",
      "Here's how you can do that in Python, simulating some data with added noise to mimic experimental data, and then using linear regression to estimate the relationship:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "\n",
      "# Generate some synthetic data: Energy levels versus quantum numbers\n",
      "np.random.seed(0)  # For reproducibility\n",
      "n = np.linspace(1, 100, 100)  # Quantum numbers from 1 to 100\n",
      "h_actual = 0.3  # Actual value of h for simulating the data\n",
      "error = np.random.normal(0, 0.05, size=n.size)  # Gaussian noise\n",
      "\n",
      "E = h_actual * n + error  # Calculating energy levels with added noise\n",
      "\n",
      "\n",
      "# Performing linear regression on the data\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(n, E)\n",
      "\n",
      "# Plotting the data points and the fitted line\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.scatter(n, E, label='Simulated data', color='blue')  # Plot the synthetic data points\n",
      "\n",
      "# Plot the linear fit\n",
      "plt.plot(n, n*slope + intercept, 'r', label=f'Fitted line: E = {slope:.2f}n + {intercept:.2f}')\n",
      "plt.legend()\n",
      "plt.xlabel('Quantum number (n)')\n",
      "plt.ylabel('Energy level (E)')\n",
      "plt.title('Linear Regression on Quantum Energy Levels')\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "\n",
      "print(f\"Estimated h (slope): {slope:.2f}\")\n",
      "print(f\"R-squared value: {r_value**2:.2f}\")  # Indicates how well the regression line fits the data\n",
      "```\n",
      "\n",
      "This code simulates the scenario presented and uses linear regression analysis to estimate the proportionality constant \\(h\\) between the quantum numbers (\\(n\\)) and their correlated energy levels (\\(E\\)), based on the assumed linear relationship handling basic quantum mechanic simulations. Remember, in real quantum mechanics applications, relationships could be more complex, integrating principles that diversely deviate from linearity depending on the system being analyzed.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "\n",
    "# Be careful with the temperature\n",
    "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template=''' \n",
    "You are an experience physicist and you are explaining quantum mechanics to a student.\n",
    "Write a short explanation about {concept}.\n",
    "'''\n",
    ")\n",
    "\n",
    "# Create an LLMChain using the first model and the prompt template\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "\n",
    "# Maybe it is interesting to have 0.5 before and 1.2 after\n",
    "llm2 = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=1.2)\n",
    "\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template=\"Given the {topic}, write a python code for a very simple case\"\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)\n",
    "\n",
    "# Combine both chains into a SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "\n",
    "# Invoke the overall chain with the concept \"linear regression\"\n",
    "output = overall_chain.invoke('linear regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context of quantum mechanics, a very simple case where we might use linear regression is to determine the relationship between the energy levels of an electron in a potential well and its quantum number. According to the basic principles of quantum mechanics, the relationship between the energy levels (E) of an electron in a simple one-dimensional potential well and their quantum numbers (n) could be modelled linearly for a specific setup (though, keep in mind, this is a simplified example for explanation purposes).\n",
      "\n",
      "Since this is a fabricated example for educational purposes, let's consider the energy levels to follow a linear relationship such as \\(E = h * n\\), where \\(E\\) is the energy, \\(n\\) is the quantum number, and \\(h\\) is a proportionality constant. Let's simulate some data for this situation and use Python's SciPy or NumPy library to perform linear regression.\n",
      "\n",
      "### Preparation\n",
      "First, ensure you have the necessary libraries. You can install `numpy` and `matplotlib` via pip if you don't have them:\n",
      "```bash\n",
      "pip install numpy matplotlib scipy\n",
      "```\n",
      "\n",
      "### Python Example\n",
      "Here's how you can do that in Python, simulating some data with added noise to mimic experimental data, and then using linear regression to estimate the relationship:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "\n",
      "# Generate some synthetic data: Energy levels versus quantum numbers\n",
      "np.random.seed(0)  # For reproducibility\n",
      "n = np.linspace(1, 100, 100)  # Quantum numbers from 1 to 100\n",
      "h_actual = 0.3  # Actual value of h for simulating the data\n",
      "error = np.random.normal(0, 0.05, size=n.size)  # Gaussian noise\n",
      "\n",
      "E = h_actual * n + error  # Calculating energy levels with added noise\n",
      "\n",
      "\n",
      "# Performing linear regression on the data\n",
      "slope, intercept, r_value, p_value, std_err = stats.linregress(n, E)\n",
      "\n",
      "# Plotting the data points and the fitted line\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.scatter(n, E, label='Simulated data', color='blue')  # Plot the synthetic data points\n",
      "\n",
      "# Plot the linear fit\n",
      "plt.plot(n, n*slope + intercept, 'r', label=f'Fitted line: E = {slope:.2f}n + {intercept:.2f}')\n",
      "plt.legend()\n",
      "plt.xlabel('Quantum number (n)')\n",
      "plt.ylabel('Energy level (E)')\n",
      "plt.title('Linear Regression on Quantum Energy Levels')\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "\n",
      "print(f\"Estimated h (slope): {slope:.2f}\")\n",
      "print(f\"R-squared value: {r_value**2:.2f}\")  # Indicates how well the regression line fits the data\n",
      "```\n",
      "\n",
      "This code simulates the scenario presented and uses linear regression analysis to estimate the proportionality constant \\(h\\) between the quantum numbers (\\(n\\)) and their correlated energy levels (\\(E\\)), based on the assumed linear relationship handling basic quantum mechanic simulations. Remember, in real quantum mechanics applications, relationships could be more complex, integrating principles that diversely deviate from linearity depending on the system being analyzed.\n"
     ]
    }
   ],
   "source": [
    "# The final response is: \n",
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Agents in Action: Python REPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Intented for demonstration/research and should be used with care. This isnt in production yet\n",
    "# pip install -q langchain_experimental "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "python_repl = PythonREPL()\n",
    "python_repl.run('print([n for n in range(1,100) if n % 13 == 0])');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo calculate the square root of the factorial of 12 and display it with 4 decimal points, I will first calculate the factorial of 12 using the `math.factorial()` function. Then, I will calculate the square root of that result using `math.sqrt()`. Finally, I will format the result to display it with 4 decimal points using the `format()` function.\n",
      "Action: Python_REPL\n",
      "Action Input: import math\n",
      "print(format(math.sqrt(math.factorial(12)), '.4f'))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m21886.1052\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 21886.1052\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the square root of the factorial of 12 and display it with 4 decimal points',\n",
       " 'output': '21886.1052'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-turbo-preview', temperature=0)\n",
    "agent_executer = create_python_agent(\n",
    "    llm=llm, \n",
    "    # Are essentially functions that agents can use for interacting with ouside world. It can vary from chain involving calculators, searches or another chains\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# \n",
    "agent_executer.invoke('Calculate the square root of the factorial of 12 and display it with 4 decimal points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to calculate 5.1 raised to the power of 7.3 to get the answer.\n",
      "Action: Python_REPL\n",
      "Action Input: print(5.1 ** 7.3)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m146306.05007233328\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 146306.05007233328\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Note the output is a dictionary containing two key pairs. \n",
    "response = agent_executer.invoke('What  is the answer to 5.1 ** 7.3?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What  is the answer to 5.1 ** 7.3?\n"
     ]
    }
   ],
   "source": [
    "print(response['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146306.05007233328\n"
     ]
    }
   ],
   "source": [
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
